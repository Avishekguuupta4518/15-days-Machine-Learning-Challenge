{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1113a3",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38265f4",
   "metadata": {},
   "source": [
    "# NLP Basics\n",
    "\n",
    "## Preprocessing Steps\n",
    "1. **Tokenization**: Splitting text into words, sentences, or subwords.\n",
    "2. **Lowercasing**: Converting all text to lowercase for consistency.\n",
    "3. **Removing punctuation and stopwords**: Cleaning irrelevant symbols and common words.\n",
    "4. **Stemming**: Reducing words to their root form (e.g., \"running\" → \"run\").\n",
    "5. **Lemmatization**: Reducing words to their dictionary form (e.g., \"better\" → \"good\").\n",
    "6. **Normalization**: Handling variations like spelling, contractions, or numbers.\n",
    "\n",
    "## Importance of Text Normalization\n",
    "- Ensures consistency across text data.\n",
    "- Reduces dimensionality of vocabulary.\n",
    "- Improves model accuracy by treating similar words as equivalent.\n",
    "\n",
    "Examples:\n",
    "- Converting “U.S.A.” → “usa”\n",
    "- Expanding contractions: “don’t” → “do not”\n",
    "- Handling numbers: “1000” → “one thousand” or “NUM”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "308c94fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['u.s.a.', 'is', 'running', 'fast', '!', 'do', \"n't\", 'worry', ',', 'it', \"'s\", '1000', 'times', 'better', '.']\n",
      "Clean tokens: ['running', 'fast', 'worry', 'times', 'better']\n",
      "Lemmas: ['run', 'fast', 'worry', 'time', 'well']\n",
      "Normalized: ['u.s.a.', 'is', 'running', 'fast', '!', 'do', \"n't\", 'worry', ',', 'it', \"'s\", 'NUM', 'times', 'better', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy English model (make sure it's installed: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"U.S.A. is running fast! Don't worry, it's 1000 times better.\"\n",
    "\n",
    "# 1. Tokenization + Lowercasing\n",
    "doc = nlp(text)\n",
    "tokens = [token.text.lower() for token in doc]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. Remove punctuation and stopwords\n",
    "tokens_clean = [token.text.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "print(\"Clean tokens:\", tokens_clean)\n",
    "\n",
    "# 3. Lemmatization (spaCy built-in)\n",
    "lemmas = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "print(\"Lemmas:\", lemmas)\n",
    "\n",
    "# 4. Normalization examples\n",
    "normalized = []\n",
    "for token in doc:\n",
    "    t = token.text.lower()\n",
    "    # Replace common patterns\n",
    "    t = re.sub(r\"\\b(u\\.s\\.a\\.)\\b\", \"usa\", t)\n",
    "    t = t.replace(\"don't\", \"do not\")\n",
    "    t = re.sub(r\"\\d+\", \"NUM\", t)  # replace numbers with NUM\n",
    "    normalized.append(t)\n",
    "print(\"Normalized:\", normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc5abc",
   "metadata": {},
   "source": [
    "# Importance of Text Normalization\n",
    "Normalization ensures that variations in text do not mislead models.\n",
    "\n",
    "Examples:\n",
    "- \"USA\", \"U.S.A.\", \"United States\" → standardized to \"usa\"\n",
    "- \"running\", \"ran\", \"runs\" → normalized to \"run\"\n",
    "- \"don't\" → \"do not\"\n",
    "\n",
    "Without normalization, models treat these as separate tokens, increasing sparsity and reducing accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d803d8",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "## Principle\n",
    "- Based on Bayes’ theorem: P(class|features) ∝ P(features|class) * P(class).\n",
    "- Assumes **conditional independence** of features given the class.\n",
    "- Works well for text classification where features are word counts.\n",
    "\n",
    "## Strengths\n",
    "- Simple, fast, efficient for high-dimensional text data.\n",
    "- Performs well with small datasets.\n",
    "- Robust to irrelevant features.\n",
    "\n",
    "## Weaknesses\n",
    "- Independence assumption often unrealistic.\n",
    "- Struggles with correlated features.\n",
    "- May underperform compared to more complex models on large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59fc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "F1-score: 0.5\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "rec.sport.baseball       0.00      0.00      0.00         0\n",
      "         sci.space       0.00      0.00      0.00         1\n",
      "     comp.graphics       1.00      1.00      1.00         1\n",
      "\n",
      "          accuracy                           0.50         2\n",
      "         macro avg       0.33      0.33      0.33         2\n",
      "      weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Synthetic text dataset\n",
    "texts = [\n",
    "    \"Baseball is a great sport\",\n",
    "    \"I love watching space launches\",\n",
    "    \"Graphics design is my passion\",\n",
    "    \"The team won the baseball game\",\n",
    "    \"NASA discovered a new planet\",\n",
    "    \"3D graphics are amazing\"\n",
    "]\n",
    "labels = [0, 1, 2, 0, 1, 2]  # 0=baseball, 1=space, 2=graphics\n",
    "categories = ['rec.sport.baseball', 'sci.space', 'comp.graphics']\n",
    "\n",
    "# Split into train/test manually\n",
    "X_train = texts[:4]\n",
    "y_train = labels[:4]\n",
    "X_test = texts[4:]\n",
    "y_test = labels[4:]\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = nb.predict(X_test_vec)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(classification_report(y_test, y_pred, target_names=categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b821dc",
   "metadata": {},
   "source": [
    "# Strengths and Weaknesses of Naive Bayes\n",
    "\n",
    "**Strengths:**\n",
    "- Fast and efficient for text classification.\n",
    "- Works well with high-dimensional sparse data.\n",
    "- Requires less training data.\n",
    "\n",
    "**Weaknesses:**\n",
    "- Assumes independence of features, which is rarely true in language.\n",
    "- Struggles with correlated words (e.g., \"New York\").\n",
    "- May be outperformed by more complex models like SVM or deep learning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
